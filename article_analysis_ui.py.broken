#!/usr/bin/env python3
"""
UI component for Article Analysis feature
"""

import os
import tempfile
import streamlit as st


def render_article_analysis():
    """Render Article Analysis tab for manuscript evaluation"""
    st.header("üìä Article Analysis - IEEE Access Submission Evaluation")
    st.markdown("""
    Upload your manuscript (Markdown or PDF) to receive a comprehensive evaluation against IEEE Access standards.
    Based on analysis of **5,634 published IEEE Access papers**.
    """)
    
    # Import analyzer
    from article_analyzer import ArticleAnalyzer
    
    # File upload
    st.divider()
    st.subheader("üìÑ Upload Manuscript")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        uploaded_file = st.file_uploader(
            "Choose your manuscript file",
            type=['md', 'pdf'],
            help="Upload a Markdown (.md) or PDF (.pdf) file"
        )
    
    with col2:
        llm_choice = st.selectbox(
            "Select LLM for Evaluation",
            ["Claude Sonnet 3.5", "Claude Opus", "OpenAI GPT-4o", "OpenAI GPT-4o-mini", "Ollama (Local)"],
            help="Choose the LLM for evaluation. Claude Sonnet 3.5 is recommended for detailed academic evaluation."
        )
    
    if not uploaded_file:
        st.info("üëÜ Upload a manuscript file to begin analysis")
        
        # Show example metrics
        st.divider()
        st.subheader("üìà IEEE Access Benchmarks (5,634 papers)")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Word Count", "6,629", help="Mean word count")
        with col2:
            st.metric("References", "42", help="Mean reference count")
        with col3:
            st.metric("Figures", "23", help="Mean figure count")
        with col4:
            st.metric("Acceptance Rate", "27%", help="IEEE Access acceptance rate")
        
        st.markdown("""
        ### üéØ Evaluation Criteria
        
        Your manuscript will be scored (1-100) on:
        
        1. **Technical Soundness** (HIGH weight) - Methodology rigor, experimental design, reproducibility
        2. **Novelty** (HIGH weight) - Clear contribution and advancement over prior work
        3. **Comprehensiveness** (MEDIUM weight) - Literature review depth, experimental coverage
        4. **Reference Quality** (MEDIUM weight) - Citation count, relevance, recent work coverage
        5. **Structure Quality** (MEDIUM weight) - Section organization, logical flow, IEEE format
        6. **Writing Quality** (MEDIUM weight) - Clarity, grammar, technical communication
        
        ### üìä Output Includes
        
        - **Overall Score** (1-100) with acceptance decision
        - **Detailed Scores** for each criterion
        - **Metrics Comparison** against IEEE Access benchmarks
        - **Strengths & Weaknesses** analysis
        - **Actionable Recommendations** for improvement
        - **Desk Rejection Risk** assessment
        """)
        
        # Comprehensive IEEE Access Information
        st.divider()
        st.subheader("üìö IEEE Access Journal Information")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **Journal Details:**
            - **ISSN**: 2169-3536
            - **Publisher**: IEEE
            - **Type**: Gold Open Access
            - **APC**: $1,950 USD
            - **Review Timeline**: 4-6 weeks (advertised), 8-12 weeks (typical)
            
            **Bibliometric Rankings (2024):**
            - **Impact Factor**: 3.6
            - **CiteScore**: 9.0
            - **H-Index**: 290
            - **SJR**: 0.849
            - **Quartile**: Q1
            """)
        
        with col2:
            st.markdown("""
            **Submission Requirements:**
            - **Abstract**: 150-250 words
            - **Keywords**: 3-4 required
            - **Page Limit**: ~20 pages recommended
            - **Format**: Double-column IEEE template
            
            **Success Patterns (from 5,634 papers):**
            - **Papers with math**: 99%
            - **Papers with statistical tests**: 91%
            - **Papers with code availability**: 20%
            - **Papers with comparisons**: 94%
            - **Papers with ablation studies**: 32%
            - **Papers with error reporting**: 59%
            """)
        
        st.divider()
        st.subheader("üéØ Acceptance Criteria & Desk Rejection Triggers")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **‚úÖ Acceptance Threshold Indicators:**
            - Clear improvement over baselines
            - Experimental validation with statistical significance
            - Comprehensive literature review
            - Reproducible methodology
            - Well-structured IEEE format
            - Novel contribution to the field
            """)
        
        with col2:
            st.markdown("""
            **üö´ Desk Rejection Triggers:**
            - Out of scope for IEEE Access
            - Below technical standards
            - No clear advance over prior work
            - Plagiarism or ethical violations
            - Severe format violations
            - Insufficient experimental validation
            """)
        
        st.divider()
        st.subheader("üìä Statistical Benchmarks from 5,634 Published Papers")
        
        import pandas as pd
        
        benchmark_data = {
            "Metric": [
                "Word Count",
                "Abstract Length",
                "References",
                "In-text Citations",
                "Refs per 1k Words",
                "Sections",
                "Avg Sentence Length",
                "Figures",
                "Tables"
            ],
            "Minimum": [3000, 1, 20, 0, 3.0, 1, 5.5, 3, 1],
            "Mean": [6629, 118, 42, 137, 6.5, 20, 18.0, 23, 14],
            "Median": [6085, 105, 38, 107, 6.5, 18, 17.5, 20, 12],
            "Maximum": [10000, 9925, 80, 1590, 12.0, 523, 97.1, 15, 8]
        }
        
        df = pd.DataFrame(benchmark_data)
        st.dataframe(df, width='stretch', hide_index=True)
        
        st.divider()
        st.subheader("üî¨ Common Research Topics & Keywords")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("""
            **Top Keywords:**
            - Machine Learning
            - Deep Learning
            - Neural Networks
            - Computer Vision
            - Natural Language Processing
            """)
        
        with col2:
            st.markdown("""
            **Successful Topics:**
            - Large Language Models (LLMs)
            - Explainable AI (XAI)
            - Federated Learning
            - Edge Computing
            - IoT & Smart Systems
            """)
        
        with col3:
            st.markdown("""
            **Common Venues Cited:**
            - Proceedings of the IEEE
            - CVPR, ICCV, ECCV
            - NeurIPS, ICML, ICLR
            - ACM Conferences
            - IEEE Transactions
            """)
        
        st.divider()
        st.subheader("üìà Citation Patterns & Reference Quality")
        
        st.markdown("""
        **Reference Year Distribution (from 225,017 citations):**
        - **Recent work (2020-2025)**: 70% of citations
        - **Foundational work (2010-2019)**: 25% of citations
        - **Classic papers (pre-2010)**: 5% of citations
        
        **Top Publisher Families:**
        1. IEEE (28%)
        2. arXiv (7%)
        3. ACM (6%)
        4. Springer (2%)
        5. NeurIPS (2%)
        
        **Citation Recency Benchmarks:**
        - **P25**: 2017
        - **P50 (Median)**: 2021
        - **P75**: 2023
        
        üí° **Tip**: Bias toward recent work (post-2017) but include foundational citations where appropriate.
        """)
        return
    
    # Process uploaded file
    st.divider()
    st.subheader("üîÑ Processing Manuscript")
    
    # Save uploaded file temporarily
    file_extension = uploaded_file.name.split('.')[-1].lower()
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_extension}') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name
    
    try:
        # Initialize analyzer
        with st.spinner("Initializing analyzer with IEEE Access standards..."):
            analyzer = ArticleAnalyzer()
        
        # Map UI choice to internal LLM type
        llm_type_map = {
            "Claude Sonnet 3.5": "claude_sonnet",
            "Claude Opus": "claude_opus",
            "OpenAI GPT-4o": "openai_gpt4o",
            "OpenAI GPT-4o-mini": "openai_gpt4o_mini",
            "Ollama (Local)": "ollama"
        }
        
        llm_type = llm_type_map.get(llm_choice, "claude_sonnet")
        
        # Run analysis
        with st.spinner(f"Analyzing manuscript with {llm_choice} (this may take 30-60 seconds)..."):
            metrics, evaluation = analyzer.analyze_article(
                tmp_path,
                file_extension,
                llm_type=llm_type
            )
        
        # Display results
        st.success("‚úÖ Analysis complete!")
        
        # Quick Summary Section
        st.divider()
        st.subheader("üìã Quick Summary")
        
        summary_col1, summary_col2 = st.columns([2, 1])
        
        with summary_col1:
            st.markdown(f"""
            **Manuscript Overview:**
            - **Word Count**: {metrics.word_count:,} words
            - **Abstract**: {metrics.abstract_length} words
            - **References**: {metrics.num_references} citations
            - **Figures**: {metrics.num_figures} | **Tables**: {metrics.num_tables}
            - **Sections**: {metrics.num_sections}
            
            **Key Findings:**
            - Overall Score: **{evaluation.overall_score}/100**
            - Decision: **{evaluation.decision}**
            - Confidence: **{evaluation.confidence}**
            - Top Strength: {evaluation.strengths[0] if evaluation.strengths else "N/A"}
            - Main Concern: {evaluation.weaknesses[0] if evaluation.weaknesses else "N/A"}
            """)
        
        with summary_col2:
            # Download buttons
            st.markdown("**üì• Download Results:**")
            
            # Generate JSON report
            import json
            from datetime import datetime
            
            report_data = {
                "analysis_date": datetime.now().isoformat(),
                "manuscript_filename": uploaded_file.name,
                "llm_used": llm_choice,
                "overall_score": evaluation.overall_score,
                "decision": evaluation.decision,
                "confidence": evaluation.confidence,
                "detailed_scores": {
                    "technical_soundness": evaluation.technical_soundness,
                    "novelty": evaluation.novelty,
                    "comprehensiveness": evaluation.comprehensiveness,
                    "reference_quality": evaluation.reference_quality,
                    "structure_quality": evaluation.structure_quality,
                    "writing_quality": evaluation.writing_quality
                },
                "metrics": {
                    "word_count": metrics.word_count,
                    "abstract_length": metrics.abstract_length,
                    "num_references": metrics.num_references,
                    "num_figures": metrics.num_figures,
                    "num_tables": metrics.num_tables,
                    "in_text_citations": metrics.in_text_citations,
                    "refs_per_1k_words": round(metrics.refs_per_1k_words, 2)
                },
                "strengths": evaluation.strengths,
                "weaknesses": evaluation.weaknesses,
                "recommendations": evaluation.recommendations,
                "desk_rejection_reasons": evaluation.desk_rejection_reasons
            }
            
            json_str = json.dumps(report_data, indent=2)
            
            st.download_button(
                label="üìÑ Download JSON",
                data=json_str,
                file_name=f"ieee_analysis_{uploaded_file.name.split('.')[0]}.json",
                mime="application/json",
                width='stretch'
            )
            
            # Generate PDF report
            from reportlab.lib.pagesizes import letter
            from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
            from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
            from reportlab.lib.enums import TA_CENTER
            from reportlab.lib import colors
            from io import BytesIO
            
            pdf_buffer = BytesIO()
            doc = SimpleDocTemplate(pdf_buffer, pagesize=letter)
            styles = getSampleStyleSheet()
            story = []
            
            # Title
            title_style = ParagraphStyle(
                'CustomTitle',
                parent=styles['Heading1'],
                fontSize=18,
                textColor=colors.HexColor('#1f77b4'),
                spaceAfter=30,
                alignment=TA_CENTER
            )
            story.append(Paragraph("IEEE Access Manuscript Evaluation Report", title_style))
            story.append(Spacer(1, 12))
            
            # Manuscript info
            story.append(Paragraph(f"<b>Manuscript:</b> {uploaded_file.name}", styles['Normal']))
            story.append(Paragraph(f"<b>Analysis Date:</b> {datetime.now().strftime('%Y-%m-%d %H:%M')}", styles['Normal']))
            story.append(Paragraph(f"<b>LLM Used:</b> {llm_choice}", styles['Normal']))
            story.append(Spacer(1, 20))
            
            # Extract and display article title and abstract
            import re
            article_preview = analyzer.parse_file(tmp_path, file_extension)
            
            # Extract title (first heading or first line)
            title_match = re.search(r'^#\s+(.+)$', article_preview, re.MULTILINE)
            if title_match:
                article_title = title_match.group(1).strip()
            else:
                # Use first non-empty line as title
                first_line = article_preview.split('\n')[0].strip()
                article_title = first_line[:100] if first_line else "Untitled"
            
            # Extract abstract
            abstract_match = re.search(r'##?\s*Abstract\s*\n(.*?)(?:\n##|\n#|\Z)', article_preview, re.IGNORECASE | re.DOTALL)
            if abstract_match:
                article_abstract = abstract_match.group(1).strip()
                # Limit abstract length for PDF
                if len(article_abstract) > 1000:
                    article_abstract = article_abstract[:1000] + "..."
            else:
                article_abstract = "No abstract found"
            
            # Add article title
            story.append(Paragraph("<b>Article Title</b>", styles['Heading2']))
            story.append(Paragraph(article_title, styles['Normal']))
            story.append(Spacer(1, 12))
            
            # Add abstract
            story.append(Paragraph("<b>Abstract</b>", styles['Heading2']))
            story.append(Paragraph(article_abstract, styles['Normal']))
            story.append(Spacer(1, 20))
            
            # Overall Evaluation
            story.append(Paragraph("<b>Overall Evaluation</b>", styles['Heading2']))
            eval_data = [
                ['Metric', 'Score/Value'],
                ['Overall Score', f'{evaluation.overall_score}/100'],
                ['Decision', evaluation.decision],
                ['Confidence', evaluation.confidence]
            ]
            eval_table = Table(eval_data, colWidths=[200, 200])
            eval_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            story.append(eval_table)
            story.append(Spacer(1, 20))
            
            # Detailed Scores
            story.append(Paragraph("<b>Detailed Scores</b>", styles['Heading2']))
            scores_data = [
                ['Criterion', 'Score'],
                ['Technical Soundness', f'{evaluation.technical_soundness}/100'],
                ['Novelty', f'{evaluation.novelty}/100'],
                ['Comprehensiveness', f'{evaluation.comprehensiveness}/100'],
                ['Reference Quality', f'{evaluation.reference_quality}/100'],
                ['Structure Quality', f'{evaluation.structure_quality}/100'],
                ['Writing Quality', f'{evaluation.writing_quality}/100']
            ]
            scores_table = Table(scores_data, colWidths=[200, 200])
            scores_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            story.append(scores_table)
            story.append(Spacer(1, 20))
            
            # Manuscript Metrics
            story.append(Paragraph("<b>Manuscript Metrics</b>", styles['Heading2']))
            metrics_data = [
                ['Metric', 'Value', 'Benchmark'],
                ['Word Count', f'{metrics.word_count:,}', '6,629 (mean)'],
                ['Abstract Length', f'{metrics.abstract_length}', '118 (mean)'],
                ['References', f'{metrics.num_references}', '42 (mean)'],
                ['In-text Citations', f'{metrics.in_text_citations}', '137 (mean)'],
                ['Figures', f'{metrics.num_figures}', '23 (mean)'],
                ['Tables', f'{metrics.num_tables}', '14 (mean)']
            ]
            metrics_table = Table(metrics_data, colWidths=[150, 125, 125])
            metrics_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            story.append(metrics_table)
            story.append(Spacer(1, 20))
            
            # Strengths
            if evaluation.strengths:
                story.append(Paragraph("<b>Strengths</b>", styles['Heading2']))
                for i, strength in enumerate(evaluation.strengths, 1):
                    story.append(Paragraph(f"{i}. {strength}", styles['Normal']))
                    story.append(Spacer(1, 6))
                story.append(Spacer(1, 12))
            
            # Weaknesses
            if evaluation.weaknesses:
                story.append(Paragraph("<b>Weaknesses</b>", styles['Heading2']))
                for i, weakness in enumerate(evaluation.weaknesses, 1):
                    story.append(Paragraph(f"{i}. {weakness}", styles['Normal']))
                    story.append(Spacer(1, 6))
                story.append(Spacer(1, 12))
            
            # Recommendations
            if evaluation.recommendations:
                story.append(Paragraph("<b>Recommendations</b>", styles['Heading2']))
                for i, rec in enumerate(evaluation.recommendations, 1):
                    story.append(Paragraph(f"{i}. {rec}", styles['Normal']))
                    story.append(Spacer(1, 6))
                story.append(Spacer(1, 12))
            
            # Desk Rejection Reasons (if any)
            if evaluation.desk_rejection_reasons:
                story.append(Paragraph("<b>Desk Rejection Risk Factors</b>", styles['Heading2']))
                for reason in evaluation.desk_rejection_reasons:
                    story.append(Paragraph(f"‚Ä¢ {reason}", styles['Normal']))
                    story.append(Spacer(1, 6))
                story.append(Spacer(1, 12))
            
            # Metrics Comparison vs Benchmarks
            story.append(Paragraph("<b>Metrics Comparison vs IEEE Access Benchmarks</b>", styles['Heading2']))
            comparison_data = [['Metric', 'Your Value', 'Benchmark Mean', 'Status']]
            for metric_name, metric_data in evaluation.metrics_comparison.items():
                status_map = {
                    "within_range": "Within Range",
                    "below_range": "Below Range",
                    "above_range": "Above Range"
                }
                comparison_data.append([
                    metric_name.replace('_', ' ').title(),
                    str(metric_data['value']),
                    str(metric_data.get('benchmark_mean', 'N/A')),
                    status_map.get(metric_data['status'], metric_data['status'])
                ])
            
            comparison_table = Table(comparison_data, colWidths=[120, 90, 90, 100])
            comparison_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 10),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black),
                ('FONTSIZE', (0, 1), (-1, -1), 9)
            ]))
            story.append(comparison_table)
            story.append(Spacer(1, 20))
            
            # Section Presence Analysis
            story.append(Paragraph("<b>Section Presence Analysis</b>", styles['Heading2']))
            section_data = [
                ['Section', 'Present'],
                ['Abstract', '‚úì' if metrics.has_abstract else '‚úó'],
                ['Introduction', '‚úì' if metrics.has_introduction else '‚úó'],
                ['Methodology', '‚úì' if metrics.has_methodology else '‚úó'],
                ['Experiments', '‚úì' if metrics.has_experiments else '‚úó'],
                ['Results', '‚úì' if metrics.has_results else '‚úó'],
                ['Discussion', '‚úì' if metrics.has_discussion else '‚úó'],
                ['Conclusion', '‚úì' if metrics.has_conclusion else '‚úó'],
                ['References', '‚úì' if metrics.has_references else '‚úó']
            ]
            section_table = Table(section_data, colWidths=[200, 200])
            section_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            story.append(section_table)
            story.append(Spacer(1, 20))
            
            # Additional Quality Indicators
            story.append(Paragraph("<b>Additional Quality Indicators</b>", styles['Heading2']))
            quality_data = [
                ['Indicator', 'Count'],
                ['Math Density', str(metrics.math_density)],
                ['Code Mentions', str(metrics.code_mentions)],
                ['Dataset Mentions', str(metrics.dataset_mentions)],
                ['Comparison Mentions', str(metrics.comparison_mentions)],
                ['Avg Sentence Length', f'{metrics.avg_sentence_length:.1f} words']
            ]
            quality_table = Table(quality_data, colWidths=[200, 200])
            quality_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            story.append(quality_table)
            
# Build PDF
doc.build(story)
pdf_bytes = pdf_buffer.getvalue()
            
st.download_button(
    label="üìë Download PDF Report",
    data=pdf_bytes,
    file_name=f"ieee_analysis_{uploaded_file.name.split('.')[0]}.pdf",
    mime="application/pdf",
    width='stretch'
)
        
# Overall score and decision
st.divider()
st.subheader("üéØ Overall Evaluation")
        
col1, col2, col3 = st.columns(3)
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # Color code based on score
            if evaluation.overall_score >= 80:
                score_color = "üü¢"
            elif evaluation.overall_score >= 60:
                score_color = "üü°"
            else:
                score_color = "üî¥"
            
            st.metric("Overall Score", f"{score_color} {evaluation.overall_score}/100")
        
        with col2:
            # Decision with emoji
            decision_emoji = {
                "Strong Accept": "üåü",
                "Accept": "‚úÖ",
                "Borderline": "‚ö†Ô∏è",
                "Reject": "‚ùå",
                "Desk Reject": "üö´"
            }
            emoji = decision_emoji.get(evaluation.decision, "")
            st.metric("Decision", f"{emoji} {evaluation.decision}")
        
        with col3:
            confidence_emoji = {"High": "üíØ", "Medium": "üéØ", "Low": "ü§î"}
            emoji = confidence_emoji.get(evaluation.confidence, "")
            st.metric("Confidence", f"{emoji} {evaluation.confidence}")
        
        # Detailed scores
        st.divider()
        st.subheader("üìä Detailed Scores")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Technical Soundness", f"{evaluation.technical_soundness}/100")
            st.metric("Novelty", f"{evaluation.novelty}/100")
        
        with col2:
            st.metric("Comprehensiveness", f"{evaluation.comprehensiveness}/100")
            st.metric("Reference Quality", f"{evaluation.reference_quality}/100")
        
        with col3:
            st.metric("Structure Quality", f"{evaluation.structure_quality}/100")
            st.metric("Writing Quality", f"{evaluation.writing_quality}/100")
        
        # Metrics comparison
        st.divider()
        st.subheader("üìà Metrics vs IEEE Access Benchmarks")
        
        import pandas as pd
        
        comparison_data = []
        for metric_name, metric_data in evaluation.metrics_comparison.items():
            status_emoji = {
                "within_range": "‚úÖ",
                "below_range": "‚¨áÔ∏è",
                "above_range": "‚¨ÜÔ∏è"
            }
            comparison_data.append({
                "Metric": metric_name.replace('_', ' ').title(),
                "Your Value": metric_data['value'],
                "Benchmark Mean": metric_data.get('benchmark_mean', 'N/A'),
                "Status": f"{status_emoji.get(metric_data['status'], '')} {metric_data['status'].replace('_', ' ').title()}"
            })
        
        df = pd.DataFrame(comparison_data)
        st.dataframe(df, width='stretch', hide_index=True)
        
        # Strengths
        if evaluation.strengths:
            st.divider()
            st.subheader("üí™ Strengths")
            for i, strength in enumerate(evaluation.strengths, 1):
                st.markdown(f"{i}. {strength}")
        
        # Weaknesses
        if evaluation.weaknesses:
            st.divider()
            st.subheader("‚ö†Ô∏è Weaknesses")
            for i, weakness in enumerate(evaluation.weaknesses, 1):
                st.markdown(f"{i}. {weakness}")
        
        # Recommendations
        if evaluation.recommendations:
            st.divider()
            st.subheader("üí° Recommendations for Improvement")
            for i, rec in enumerate(evaluation.recommendations, 1):
                st.markdown(f"{i}. {rec}")
        
        # Desk rejection reasons (if any)
        if evaluation.desk_rejection_reasons:
            st.divider()
            st.error("üö´ Desk Rejection Risk Factors")
            for reason in evaluation.desk_rejection_reasons:
                st.markdown(f"- {reason}")
        
        # Extracted metrics (expandable)
        st.divider()
        with st.expander("üîç Detailed Extracted Metrics"):
            st.json({
                "word_count": metrics.word_count,
                "abstract_length": metrics.abstract_length,
                "num_sections": metrics.num_sections,
                "num_references": metrics.num_references,
                "num_figures": metrics.num_figures,
                "num_tables": metrics.num_tables,
                "in_text_citations": metrics.in_text_citations,
                "refs_per_1k_words": round(metrics.refs_per_1k_words, 2),
                "math_density": metrics.math_density,
                "code_mentions": metrics.code_mentions,
                "dataset_mentions": metrics.dataset_mentions,
                "comparison_mentions": metrics.comparison_mentions,
                "avg_sentence_length": round(metrics.avg_sentence_length, 1),
                "section_presence": {
                    "abstract": metrics.has_abstract,
                    "introduction": metrics.has_introduction,
                    "methodology": metrics.has_methodology,
                    "experiments": metrics.has_experiments,
                    "results": metrics.has_results,
                    "discussion": metrics.has_discussion,
                    "conclusion": metrics.has_conclusion,
                    "references": metrics.has_references
                }
            })
        
    except Exception as e:
        st.error(f"‚ùå Error analyzing manuscript: {str(e)}")
        import traceback
        with st.expander("Error details"):
            st.code(traceback.format_exc())
    
    finally:
        # Clean up temp file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass
